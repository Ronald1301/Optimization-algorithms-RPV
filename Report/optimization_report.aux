\relax 
\providecommand \babel@aux [2]{\global \let \babel@toc \@gobbletwo }
\@nameuse{bbl@beforestart}
\catcode `"\active 
\catcode `<\active 
\catcode `>\active 
\@nameuse{es@quoting}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{spanish}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introducción}{2}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Contexto del Problema}{2}{subsection.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graficación de la función en 3D.}}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:function_3d}{{1}{2}{Graficación de la función en 3D}{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Curvas de nivel}}{3}{figure.caption.2}\protected@file@percent }
\newlabel{fig:function_contour}{{2}{3}{Curvas de nivel}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Características Notables de la Función}{3}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Objetivos del Análisis}{3}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Análisis Teórico del Modelo}{4}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existencia y Unicidad del Mínimo}{4}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Análisis de Puntos Críticos}{4}{subsection.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Cálculo del Gradiente}{4}{subsubsection.2.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Análisis de la Hessiana}{4}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Expresión General de la Hessiana}{4}{subsubsection.2.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Comportamiento en el Origen}{5}{subsubsection.2.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Análisis de Convexidad}{5}{subsection.2.4}\protected@file@percent }
\newlabel{def:funcion_convexa}{{2.1}{5}{Función convexa}{definicion.2.1}{}}
\newlabel{teo:caracterizacion_convexidad}{{2.2}{5}{Caracterización de convexidad para funciones $C^2$}{teorema.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Proposición sobre la Convexidad de $f(x,y)$}{5}{subsubsection.2.4.1}\protected@file@percent }
\newlabel{prop:convexidad_funcion}{{2.3}{5}{Convexidad de $f(x,y)$}{proposicion.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Ejemplo: Demostración de No Convexidad mediante la Hessiana}{5}{subsubsection.2.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Importancia de la Convexidad en PNL}{6}{subsubsection.2.4.3}\protected@file@percent }
\newlabel{teo:importancia_convexidad}{{2.3}{6}{Importancia de la convexidad en PNL}{teorema.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Corolario para Nuestra Función}{6}{subsubsection.2.4.4}\protected@file@percent }
\newlabel{cor:implicaciones_convexidad}{{2.1}{6}{}{corolario.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Algoritmos de Optimización Implementados}{7}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Marco Teórico de Algoritmos de Optimización}{7}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1}Algoritmos de Búsqueda Direccional}{7}{subsubsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2}Reglas de Búsqueda Lineal (Armijo)}{7}{subsubsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.3}Método de Máximo Descenso}{7}{subsubsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.4}Método de Newton}{7}{subsubsection.3.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Filosofía de la Selección de Algoritmos: Un Enfoque Teórico-Práctico}{7}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Base Teórica para la Selección}{8}{subsubsection.3.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Justificación Teórica de Cada Algoritmo}{8}{subsubsection.3.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.3}Complementariedad Teórica de los Algoritmos}{8}{subsubsection.3.2.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Caracterización teórica de los algoritmos seleccionados}}{9}{table.caption.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Gradiente Descendente}{9}{subsection.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Formulación Matemática}{9}{subsubsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Variante de Paso Fijo}{9}{subsubsection.3.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Variante Adaptativa}{9}{subsubsection.3.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Método de Newton}{9}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Formulación Clásica}{9}{subsubsection.3.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.2}Mejoras Implementadas}{10}{subsubsection.3.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Algoritmo Híbrido}{10}{subsection.3.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Lógica de Conmutación}{10}{subsubsection.3.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Justificación del Enfoque}{10}{subsubsection.3.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimentos Numéricos}{10}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Diseño Experimental Exhaustivo}{10}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Conjunto Completo de Puntos Iniciales}{10}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Caracterización del conjunto completo de puntos iniciales}}{10}{table.caption.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Clasificación por Distancia al Óptimo}{10}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Clasificación de puntos iniciales por distancia al óptimo}}{11}{table.caption.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Métricas de Evaluación Estadística}{11}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Análisis de Convergencia desde Punto Específico}{12}{subsection.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Convergencia del valor de la función y norma del gradiente desde el punto (10,10). Se observa la rápida convergencia de Newton con decrecimiento súper-lineal del valor de la función, consistente con la teoría de convergencia de Newton. El método de Gradiente Descendente muestra decrecimiento lineal, como predice la teoría para el método de máximo descenso. La norma del gradiente decrece más rápidamente en Newton, indicando mayor eficiencia en alcanzar puntos estacionarios. El algoritmo híbrido mantiene un comportamiento estable combinando ambas estrategias.}}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:convergence_10_10}{{3}{12}{Convergencia del valor de la función y norma del gradiente desde el punto (10,10). Se observa la rápida convergencia de Newton con decrecimiento súper-lineal del valor de la función, consistente con la teoría de convergencia de Newton. El método de Gradiente Descendente muestra decrecimiento lineal, como predice la teoría para el método de máximo descenso. La norma del gradiente decrece más rápidamente en Newton, indicando mayor eficiencia en alcanzar puntos estacionarios. El algoritmo híbrido mantiene un comportamiento estable combinando ambas estrategias}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Resultados Estadísticos Completos}{12}{subsection.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.1}Estadísticas Descriptivas por Algoritmo}{12}{subsubsection.4.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Estadísticas completas de iteraciones (441 experimentos)}}{12}{table.caption.7}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Estadísticas de tiempo de ejecución en segundos (441 experimentos)}}{12}{table.caption.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.4.2}Análisis de Precisión y Tasa de Éxito}{13}{subsubsection.4.4.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces Análisis de precisión y robustez (441 experimentos)}}{13}{table.caption.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Análisis por Categorías de Distancia}{13}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.1}Comportamiento Según Distancia al Óptimo}{13}{subsubsection.4.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces Iteraciones promedio por categoría de distancia}}{13}{table.caption.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.5.2}Análisis de Fallos por Categoría}{13}{subsubsection.4.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces Distribución de fallos por algoritmo y distancia}}{13}{table.caption.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Análisis de Robustez}{14}{subsection.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Análisis de robustez: iteraciones y precisión vs distancia inicial al óptimo. El algoritmo híbrido muestra el mejor balance entre eficiencia y robustez.}}{14}{figure.caption.12}\protected@file@percent }
\newlabel{fig:robustness_analysis}{{4}{14}{Análisis de robustez: iteraciones y precisión vs distancia inicial al óptimo. El algoritmo híbrido muestra el mejor balance entre eficiencia y robustez}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Análisis de Precisión y Eficiencia Computacional}{15}{subsection.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Distribución de precisión por algoritmo (solo experimentos exitosos). Newton muestra la mayor precisión con distribución más concentrada, mientras GD Paso Fijo tiene mayor variabilidad.}}{15}{figure.caption.13}\protected@file@percent }
\newlabel{fig:histograma_precision}{{5}{15}{Distribución de precisión por algoritmo (solo experimentos exitosos). Newton muestra la mayor precisión con distribución más concentrada, mientras GD Paso Fijo tiene mayor variabilidad}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Relación entre iteraciones y tiempo de ejecución. Se observa correlación lineal positiva, con Newton mostrando menor pendiente debido a su mayor costo computacional por iteración.}}{16}{figure.caption.14}\protected@file@percent }
\newlabel{fig:tiempo_vs_iteraciones}{{6}{16}{Relación entre iteraciones y tiempo de ejecución. Se observa correlación lineal positiva, con Newton mostrando menor pendiente debido a su mayor costo computacional por iteración}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.1}Análisis de la Relación Iteraciones-Tiempo}{16}{subsubsection.4.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.2}Distribución de Precisión por Algoritmo}{16}{subsubsection.4.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Visualización de Resultados}{17}{subsection.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Diagrama de caja de iteraciones por algoritmo (441 experimentos). Se observa la menor variabilidad de Newton y la robustez del GD Adaptativo.}}{17}{figure.caption.15}\protected@file@percent }
\newlabel{fig:boxplot_iteraciones}{{7}{17}{Diagrama de caja de iteraciones por algoritmo (441 experimentos). Se observa la menor variabilidad de Newton y la robustez del GD Adaptativo}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Comparativa del rendimiento de los algoritmos de optimización seleccionados.}}{18}{figure.caption.16}\protected@file@percent }
\newlabel{fig:massive_experiment_results}{{8}{18}{Comparativa del rendimiento de los algoritmos de optimización seleccionados}{figure.caption.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Mapa de calor de tasa de éxito por coordenadas iniciales. Las regiones rojas indican fallos de convergencia, concentrados cerca del origen para Newton y distribuidas aleatoriamente para GD Paso Fijo.}}{19}{figure.caption.17}\protected@file@percent }
\newlabel{fig:heatmap_convergencia}{{9}{19}{Mapa de calor de tasa de éxito por coordenadas iniciales. Las regiones rojas indican fallos de convergencia, concentrados cerca del origen para Newton y distribuidas aleatoriamente para GD Paso Fijo}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Interpretación de Resultados del Experimento Masivo}{19}{subsection.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.9.1}Hallazgos Principales del Análisis Estadístico}{19}{subsubsection.4.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.9.2}Análisis de Patrones Espaciales}{19}{subsubsection.4.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.9.3}Implicaciones Prácticas}{20}{subsubsection.4.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Comparación con el Experimento Original}{20}{subsection.4.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces Comparación entre experimento inicial y masivo}}{20}{table.caption.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.1}Tabla de Comparación Completa}{21}{subsubsection.4.10.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {10}{\ignorespaces Comparación completa de algoritmos en todas las métricas (441 experimentos)}}{21}{table.caption.19}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Implementación del Código}{22}{appendix.Alph1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Estructura del Código}{22}{subsection.Alph1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Dependencias y Requisitos}{22}{subsection.Alph1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Reproducibilidad}{22}{subsection.Alph1.3}\protected@file@percent }
\gdef \@abspage@last{22}
