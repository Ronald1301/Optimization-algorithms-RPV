{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d96ccb4",
   "metadata": {},
   "source": [
    "# Análisis de Optimización: Implementaciones Extras\n",
    "\n",
    "Este notebook contiene implementaciones adicionales y análisis complementarios para el estudio de la función $f(x, y) = \\log(x^2 + y^2 + 1) \\cdot \\arctan(x^2 + y^2)$.\n",
    "\n",
    "**Contenido:**\n",
    "1. Métodos Quasi-Newton (BFGS, DFP)\n",
    "2. Algoritmos de Región de Confianza\n",
    "3. Métodos de Penalidad para Problemas con Restricciones\n",
    "4. Análisis de Sensibilidad Avanzado\n",
    "5. Visualizaciones Extendidas\n",
    "6. Experimentos Adicionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866426d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumba\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m jit\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m     12\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Configuración para mejores gráficas\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize, BFGS, NonlinearConstraint\n",
    "import time\n",
    "import warnings\n",
    "from numba import jit\n",
    "import pandas as pd\n",
    "# sklearn es opcional para algunas utilidades; manejar ausencia gracilmente\n",
    "try:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except Exception:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    StandardScaler = None\n",
    "    print(\"Warning: scikit-learn not installed. To enable optional utilities install it with: pip install scikit-learn\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración para mejores gráficas\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820e8a95",
   "metadata": {},
   "source": [
    "## 1. Métodos Quasi-Newton\n",
    "\n",
    "Implementaciones de BFGS y DFP como funciones reutilizables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfgs_method(f, grad_f, x0, max_iter=1000, tol=1e-8):\n",
    "    \"\"\"Implementación del método BFGS (Broyden-Fletcher-Goldfarb-Shanno)\n",
    "    - Aproximación de la Hessiana usando actualizaciones de rango 2\n",
    "    - Convergencia superlineal\n",
    "    - No requiere cálculo explícito de la Hessiana\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    n = len(x)\n",
    "    H = np.eye(n)\n",
    "    trajectory = [x.copy()]\n",
    "    for i in range(max_iter):\n",
    "        grad = np.array(grad_f(x[0], x[1]))\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        d = -H @ grad\n",
    "        # Búsqueda lineal (backtracking)\n",
    "        alpha = 1.0\n",
    "        rho = 0.5\n",
    "        c = 1e-4\n",
    "        for _ in range(20):\n",
    "            x_new = x + alpha * d\n",
    "            if f(x_new[0], x_new[1]) <= f(x[0], x[1]) + c * alpha * grad @ d:\n",
    "                break\n",
    "            alpha *= rho\n",
    "        x_old = x.copy()\n",
    "        x = x + alpha * d\n",
    "        s = x - x_old\n",
    "        grad_new = np.array(grad_f(x[0], x[1]))\n",
    "        y = grad_new - grad\n",
    "        if np.abs(s @ y) > 1e-12:\n",
    "            rho_bfgs = 1.0 / (s @ y)\n",
    "            I = np.eye(n)\n",
    "            H = (I - rho_bfgs * np.outer(s, y)) @ H @ (I - rho_bfgs * np.outer(y, s)) + rho_bfgs * np.outer(s, s)\n",
    "        trajectory.append(x.copy())\n",
    "    return x, trajectory\n",
    "\n",
    "def dfp_method(f, grad_f, x0, max_iter=1000, tol=1e-8):\n",
    "    \"\"\"Implementación del método DFP (Davidon-Fletcher-Powell)\n",
    "    - Aproximación de la inversa de la Hessiana\n",
    "    - Convergencia superlineal\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    n = len(x)\n",
    "    B_inv = np.eye(n)\n",
    "    trajectory = [x.copy()]\n",
    "    for i in range(max_iter):\n",
    "        grad = np.array(grad_f(x[0], x[1]))\n",
    "        grad_norm = np.linalg.norm(grad)\n",
    "        if grad_norm < tol:\n",
    "            break\n",
    "        d = -B_inv @ grad\n",
    "        alpha = 1.0\n",
    "        rho = 0.5\n",
    "        c = 1e-4\n",
    "        for _ in range(20):\n",
    "            x_new = x + alpha * d\n",
    "            if f(x_new[0], x_new[1]) <= f(x[0], x[1]) + c * alpha * grad @ d:\n",
    "                break\n",
    "            alpha *= rho\n",
    "        x_old = x.copy()\n",
    "        x = x + alpha * d\n",
    "        s = x - x_old\n",
    "        grad_new = np.array(grad_f(x[0], x[1]))\n",
    "        y = grad_new - grad\n",
    "        if np.abs(s @ y) > 1e-12:\n",
    "            B_inv = B_inv + np.outer(s, s) / (s @ y) - (B_inv @ np.outer(y, y) @ B_inv) / (y @ B_inv @ y)\n",
    "        trajectory.append(x.copy())\n",
    "    return x, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d502c709",
   "metadata": {},
   "source": [
    "## 2. Métodos de Región de Confianza\n",
    "\n",
    "Implementación del método Dogleg y utilidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3c85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tau_dogleg(p_cauchy, p_newton, delta):\n",
    "    u = p_cauchy\n",
    "    v = p_newton - p_cauchy\n",
    "    a = v @ v\n",
    "    b = 2 * (u @ v)\n",
    "    c = u @ u - delta ** 2\n",
    "    discriminant = b ** 2 - 4 * a * c\n",
    "    if discriminant < 0:\n",
    "        return 1.0\n",
    "    tau1 = (-b + np.sqrt(discriminant)) / (2 * a)\n",
    "    tau2 = (-b - np.sqrt(discriminant)) / (2 * a)\n",
    "    valid_taus = [tau for tau in [tau1, tau2] if 0 <= tau <= 1]\n",
    "    if valid_taus:\n",
    "        return max(valid_taus)\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "def trust_region_dogleg(f, grad_f, hess_f, x0, max_iter=100, tol=1e-8, delta0=1.0):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    delta = delta0\n",
    "    eta = 0.1\n",
    "    trajectory = [x.copy()]\n",
    "    for i in range(max_iter):\n",
    "        grad = np.array(grad_f(x[0], x[1]))\n",
    "        H = hess_f(x[0], x[1])\n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            break\n",
    "        eigenvalues = np.linalg.eigvals(H)\n",
    "        if np.min(eigenvalues) < 1e-8:\n",
    "            H_reg = H + 1e-8 * np.eye(2)\n",
    "        else:\n",
    "            H_reg = H\n",
    "        try:\n",
    "            p_newton = -np.linalg.solve(H_reg, grad)\n",
    "        except np.linalg.LinAlgError:\n",
    "            p_newton = -np.linalg.pinv(H_reg) @ grad\n",
    "        p_cauchy = - (grad @ grad) / (grad @ H @ grad) * grad\n",
    "        if np.linalg.norm(p_newton) <= delta:\n",
    "            p = p_newton\n",
    "        elif np.linalg.norm(p_cauchy) >= delta:\n",
    "            p = (delta / np.linalg.norm(p_cauchy)) * p_cauchy\n",
    "        else:\n",
    "            tau = find_tau_dogleg(p_cauchy, p_newton, delta)\n",
    "            p = p_cauchy + tau * (p_newton - p_cauchy)\n",
    "        actual_reduction = f(x[0], x[1]) - f(x[0] + p[0], x[1] + p[1])\n",
    "        predicted_reduction = -grad @ p - 0.5 * p @ H @ p\n",
    "        if predicted_reduction == 0:\n",
    "            rho = 0\n",
    "        else:\n",
    "            rho = actual_reduction / predicted_reduction\n",
    "        if rho < 0.25:\n",
    "            delta *= 0.25\n",
    "        elif rho > 0.75 and np.linalg.norm(p) == delta:\n",
    "            delta = min(2 * delta, 10.0)\n",
    "        if rho > eta:\n",
    "            x = x + p\n",
    "        trajectory.append(x.copy())\n",
    "    return x, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8cd331",
   "metadata": {},
   "source": [
    "## 3. Métodos de Penalidad para Problemas con Restricciones\n",
    "\n",
    "Penalidad cuadrática y Lagrangiano aumentado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadratic_penalty_method(f, grad_f, constraints, x0, mu0=1.0, max_iter=100, tol=1e-6):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    mu = mu0\n",
    "    trajectory = [x.copy()]\n",
    "    penalty_parameters = [mu]\n",
    "    for k in range(max_iter):\n",
    "        def P(x_val):\n",
    "            penalty = 0\n",
    "            for h, _ in constraints:\n",
    "                penalty += h(x_val[0], x_val[1]) ** 2\n",
    "            return f(x_val[0], x_val[1]) + mu * penalty\n",
    "        def grad_P(x_val):\n",
    "            grad_penalty = np.zeros(2)\n",
    "            for h, grad_h in constraints:\n",
    "                h_val = h(x_val[0], x_val[1])\n",
    "                grad_h_val = np.array(grad_h(x_val[0], x_val[1]))\n",
    "                grad_penalty += 2 * h_val * grad_h_val\n",
    "            grad_f_val = np.array(grad_f(x_val[0], x_val[1]))\n",
    "            return grad_f_val + mu * grad_penalty\n",
    "        x_opt, _ = bfgs_method(lambda x, y: P([x, y]), \n",
    "                              lambda x, y: grad_P([x, y]), \n",
    "                              x, max_iter=1000, tol=1e-6)\n",
    "        max_constraint_violation = 0\n",
    "        for h, _ in constraints:\n",
    "            max_constraint_violation = max(max_constraint_violation, \n",
    "                                         abs(h(x_opt[0], x_opt[1])))\n",
    "        if max_constraint_violation < tol:\n",
    "            break\n",
    "        mu *= 10\n",
    "        x = x_opt\n",
    "        trajectory.append(x.copy())\n",
    "    return x, trajectory, penalty_parameters\n",
    "\n",
    "def augmented_lagrangian(f, grad_f, constraints, x0, lambda0=None, mu0=1.0, \n",
    "                        max_iter=100, tol=1e-6):\n",
    "    x = np.array(x0, dtype=float)\n",
    "    mu = mu0\n",
    "    if lambda0 is None:\n",
    "        lambda_vec = np.zeros(len(constraints))\n",
    "    else:\n",
    "        lambda_vec = np.array(lambda0)\n",
    "    trajectory = [x.copy()]\n",
    "    lambda_history = [lambda_vec.copy()]\n",
    "    for k in range(max_iter):\n",
    "        def L_aug(x_val):\n",
    "            penalty = 0\n",
    "            for i, (h, _) in enumerate(constraints):\n",
    "                h_val = h(x_val[0], x_val[1])\n",
    "                penalty += lambda_vec[i] * h_val + (mu/2) * h_val ** 2\n",
    "            return f(x_val[0], x_val[1]) + penalty\n",
    "        def grad_L_aug(x_val):\n",
    "            grad_penalty = np.zeros(2)\n",
    "            for i, (h, grad_h) in enumerate(constraints):\n",
    "                h_val = h(x_val[0], x_val[1])\n",
    "                grad_h_val = np.array(grad_h(x_val[0], x_val[1]))\n",
    "                grad_penalty += (lambda_vec[i] + mu * h_val) * grad_h_val\n",
    "            grad_f_val = np.array(grad_f(x_val[0], x_val[1]))\n",
    "            return grad_f_val + grad_penalty\n",
    "        x_opt, _ = bfgs_method(lambda x, y: L_aug([x, y]), \n",
    "                              lambda x, y: grad_L_aug([x, y]), \n",
    "                              x, max_iter=1000, tol=1e-6)\n",
    "        for i, (h, _) in enumerate(constraints):\n",
    "            lambda_vec[i] += mu * h(x_opt[0], x_opt[1])\n",
    "        max_constraint_violation = 0\n",
    "        for h, _ in constraints:\n",
    "            max_constraint_violation = max(max_constraint_violation, \n",
    "                                         abs(h(x_opt[0], x_opt[1])))\n",
    "        if max_constraint_violation < tol:\n",
    "            break\n",
    "        if max_constraint_violation > 0.25 * tol:\n",
    "            mu *= 10\n",
    "        x = x_opt\n",
    "        trajectory.append(x.copy())\n",
    "        lambda_history.append(lambda_vec.copy())\n",
    "    return x, trajectory, lambda_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85e725",
   "metadata": {},
   "source": [
    "## 4. Análisis de Sensibilidad Avanzado\n",
    "\n",
    "Funciones auxiliares para analizar la sensibilidad del óptimo ante parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec9a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(f, grad_f, hess_f, x_opt, parameters, delta=0.01):\n",
    "    H = hess_f(x_opt[0], x_opt[1])\n",
    "    if np.linalg.cond(H) > 1e12:\n",
    "        print(\"Advertencia: Hessiana mal condicionada, usando pseudo-inversa\")\n",
    "        H_inv = np.linalg.pinv(H)\n",
    "    else:\n",
    "        H_inv = np.linalg.inv(H)\n",
    "    sensitivities = {}\n",
    "    for param_name, param_value in parameters.items():\n",
    "        if param_name == 'learning_rate':\n",
    "            grad_perturbed = finite_difference_gradient(f, x_opt, delta, param_value)\n",
    "        elif param_name == 'regularization':\n",
    "            grad_perturbed = finite_difference_regularization(f, x_opt, delta, param_value)\n",
    "        else:\n",
    "            grad_perturbed = np.zeros(2)\n",
    "        sensitivity = -H_inv @ grad_perturbed\n",
    "        sensitivities[param_name] = sensitivity\n",
    "    return sensitivities\n",
    "\n",
    "def finite_difference_gradient(f, x, delta, learning_rate):\n",
    "    grad_plus = np.zeros(2)\n",
    "    for i in range(2):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += delta\n",
    "        x_next_plus = x_plus - (learning_rate + delta) * np.array([1, 1])  # Gradiente simulado\n",
    "        grad_plus[i] = (f(x_next_plus[0], x_next_plus[1]) - f(x[0], x[1])) / delta\n",
    "    return grad_plus\n",
    "\n",
    "def finite_difference_regularization(f, x, delta, regularization):\n",
    "    grad_plus = np.zeros(2)\n",
    "    for i in range(2):\n",
    "        x_plus = x.copy()\n",
    "        x_plus[i] += delta\n",
    "        reg_effect_plus = regularization * np.linalg.norm(x_plus) ** 2\n",
    "        reg_effect = regularization * np.linalg.norm(x) ** 2\n",
    "        grad_plus[i] = ((f(x_plus[0], x_plus[1]) + reg_effect_plus) - \n",
    "                       (f(x[0], x[1]) + reg_effect)) / delta\n",
    "    return grad_plus\n",
    "\n",
    "def convergence_rate_analysis(trajectories, algorithm_names):\n",
    "    convergence_data = {}\n",
    "    for name, trajectory in zip(algorithm_names, trajectories):\n",
    "        errors = []\n",
    "        for point in trajectory:\n",
    "            error = np.linalg.norm(point - np.array([0, 0]))\n",
    "            errors.append(error)\n",
    "        if len(errors) > 1:\n",
    "            rates = []\n",
    "            for i in range(1, len(errors)):\n",
    "                if errors[i-1] > 1e-12:\n",
    "                    rate = errors[i] / errors[i-1]\n",
    "                    rates.append(rate)\n",
    "            avg_rate = np.mean(rates) if rates else 0\n",
    "            std_rate = np.std(rates) if rates else 0\n",
    "        else:\n",
    "            avg_rate = 0\n",
    "            std_rate = 0\n",
    "        convergence_data[name] = {\n",
    "            'final_error': errors[-1] if errors else float('inf'),\n",
    "            'iterations': len(trajectory),\n",
    "            'avg_convergence_rate': avg_rate,\n",
    "            'std_convergence_rate': std_rate,\n",
    "            'errors': errors\n",
    "        }\n",
    "    return convergence_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5714db4",
   "metadata": {},
   "source": [
    "## 5. Visualizaciones Extendidas\n",
    "\n",
    "Funciones para comparar convergencia y visualizar paisajes 3D con trayectorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80433ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_convergence_comparison(convergence_data):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    for name, data in convergence_data.items():\n",
    "        axes[0, 0].semilogy(data['errors'], label=name, linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Iteración')\n",
    "    axes[0, 0].set_ylabel('Error (log scale)')\n",
    "    axes[0, 0].set_title('Convergencia de Algoritmos')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    names = list(convergence_data.keys())\n",
    "    avg_rates = [convergence_data[name]['avg_convergence_rate'] for name in names]\n",
    "    std_rates = [convergence_data[name]['std_convergence_rate'] for name in names]\n",
    "    bars = axes[0, 1].bar(names, avg_rates, yerr=std_rates, capsize=5, alpha=0.7)\n",
    "    axes[0, 1].set_ylabel('Tasa de Convergencia Promedio')\n",
    "    axes[0, 1].set_title('Análisis de Tasas de Convergencia')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    for bar, rate in zip(bars, avg_rates):\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01, f'{rate:.3f}', ha='center', va='bottom')\n",
    "    iterations = [convergence_data[name]['iterations'] for name in names]\n",
    "    final_errors = [convergence_data[name]['final_error'] for name in names]\n",
    "    scatter = axes[1, 0].scatter(iterations, final_errors, s=100, alpha=0.7)\n",
    "    for i, name in enumerate(names):\n",
    "        axes[1, 0].annotate(name, (iterations[i], final_errors[i]), xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1, 0].set_xlabel('Número de Iteraciones')\n",
    "    axes[1, 0].set_ylabel('Error Final')\n",
    "    axes[1, 0].set_title('Eficiencia vs Precisión')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    sensitivity_matrix = np.array([ [convergence_data[name]['avg_convergence_rate'], convergence_data[name]['std_convergence_rate']] for name in names])\n",
    "    im = axes[1, 1].imshow(sensitivity_matrix, cmap='viridis', aspect='auto')\n",
    "    axes[1, 1].set_xticks([0, 1])\n",
    "    axes[1, 1].set_xticklabels(['Tasa Promedio', 'Desviación'])\n",
    "    axes[1, 1].set_yticks(range(len(names)))\n",
    "    axes[1, 1].set_yticklabels(names)\n",
    "    axes[1, 1].set_title('Mapa de Sensibilidad')\n",
    "    for i in range(len(names)):\n",
    "        for j in range(2):\n",
    "            axes[1, 1].text(j, i, f'{sensitivity_matrix[i, j]:.3f}', ha=\"center\", va=\"center\", color=\"w\")\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_3d_landscape_with_trajectories(f, trajectories, algorithm_names, x_range=(-2, 2), y_range=(-2, 2)):\n",
    "    fig = plt.figure(figsize=(15, 10))\n",
    "    x = np.linspace(x_range[0], x_range[1], 50)\n",
    "    y = np.linspace(y_range[0], y_range[1], 50)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = f(X, Y)\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.7, linewidth=0, antialiased=True)\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    for i, (trajectory, name) in enumerate(zip(trajectories, algorithm_names)):\n",
    "        if len(trajectory) > 0:\n",
    "            x_vals = [point[0] for point in trajectory]\n",
    "            y_vals = [point[1] for point in trajectory]\n",
    "            z_vals = [f(point[0], point[1]) for point in trajectory]\n",
    "            ax1.plot(x_vals, y_vals, z_vals, color=colors[i % len(colors)], linewidth=3, marker='o', markersize=4, label=name)\n",
    "    ax1.set_xlabel('X')\n",
    "    ax1.set_ylabel('Y')\n",
    "    ax1.set_zlabel('f(X, Y)')\n",
    "    ax1.set_title('Paisaje 3D con Trayectorias de Optimización')\n",
    "    ax1.legend()\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    contour = ax2.contour(X, Y, Z, levels=20, alpha=0.6)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    for i, (trajectory, name) in enumerate(zip(trajectories, algorithm_names)):\n",
    "        if len(trajectory) > 0:\n",
    "            x_vals = [point[0] for point in trajectory]\n",
    "            y_vals = [point[1] for point in trajectory]\n",
    "            ax2.plot(x_vals, y_vals, color=colors[i % len(colors)], linewidth=2, marker='o', markersize=4, label=name)\n",
    "    ax2.set_xlabel('X')\n",
    "    ax2.set_ylabel('Y')\n",
    "    ax2.set_title('Mapa de Contorno con Trayectorias')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd62b80",
   "metadata": {},
   "source": [
    "## 6. Experimentos Adicionales\n",
    "\n",
    "Funciones para ejecutar experimentos extendidos y con restricciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afef0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extended_experiments(f, grad_f, hess_f):\n",
    "    extended_test_points = [ (15, -25), (-30, -40), (0.001, 0.001), (75, -75), (-0.1, 0.2), (200, 50) ]\n",
    "    all_results = {}\n",
    "    for point in extended_test_points:\n",
    "        print(f\"\\n=== Experimentos desde punto: {point} ===\")\n",
    "        point_results = {}\n",
    "        start_time = time.time()\n",
    "        x_opt_bfgs, traj_bfgs = bfgs_method(f, grad_f, point)\n",
    "        bfgs_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        x_opt_dfp, traj_dfp = dfp_method(f, grad_f, point)\n",
    "        dfp_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        x_opt_trust, traj_trust = trust_region_dogleg(f, grad_f, hess_f, point)\n",
    "        trust_time = time.time() - start_time\n",
    "        point_results['BFGS'] = {'x_opt': x_opt_bfgs, 'trajectory': traj_bfgs, 'time': bfgs_time, 'iterations': len(traj_bfgs), 'final_value': f(x_opt_bfgs[0], x_opt_bfgs[1]) }\n",
    "        point_results['DFP'] = {'x_opt': x_opt_dfp, 'trajectory': traj_dfp, 'time': dfp_time, 'iterations': len(traj_dfp), 'final_value': f(x_opt_dfp[0], x_opt_dfp[1]) }\n",
    "        point_results['Trust Region'] = {'x_opt': x_opt_trust, 'trajectory': traj_trust, 'time': trust_time, 'iterations': len(traj_trust), 'final_value': f(x_opt_trust[0], x_opt_trust[1]) }\n",
    "        all_results[point] = point_results\n",
    "        for method, results in point_results.items():\n",
    "            success = \"ÉXITO\" if results['final_value'] < 1e-6 else \"FALLO\"\n",
    "            print(f\"  {method:15}: {results['iterations']:3d} iter, {results['time']:.4f}s, f(x) = {results['final_value']:.2e} [{success}]\")\n",
    "    return all_results\n",
    "\n",
    "def constrained_optimization_experiments(f, grad_f):\n",
    "    def constraint1(x, y):\n",
    "        return x + y - 1\n",
    "    def grad_constraint1(x, y):\n",
    "        return np.array([1, 1])\n",
    "    def constraint2(x, y):\n",
    "        return x**2 + y**2 - 4\n",
    "    def grad_constraint2(x, y):\n",
    "        return np.array([2*x, 2*y])\n",
    "    constraints = [(constraint1, grad_constraint1), (constraint2, grad_constraint2)]\n",
    "    constrained_points = [(3, 3), (-1, -1), (0, 2), (2, 0)]\n",
    "    constrained_results = {}\n",
    "    for point in constrained_points:\n",
    "        print(f\"\\n=== Optimización con Restricciones desde: {point} ===\")\n",
    "        x_opt_penalty, traj_penalty, mu_history = quadratic_penalty_method(f, grad_f, constraints, point)\n",
    "        x_opt_auglag, traj_auglag, lambda_history = augmented_lagrangian(f, grad_f, constraints, point)\n",
    "        penalty_feasible = all(abs(constraint(x_opt_penalty[0], x_opt_penalty[1])) < 1e-6 for constraint, _ in constraints)\n",
    "        auglag_feasible = all(abs(constraint(x_opt_auglag[0], x_opt_auglag[1])) < 1e-6 for constraint, _ in constraints)\n",
    "        constrained_results[point] = {'Penalty': {'x_opt': x_opt_penalty, 'feasible': penalty_feasible, 'time': None, 'iterations': len(traj_penalty), 'final_value': f(x_opt_penalty[0], x_opt_penalty[1])},\n",
    "                                    'Augmented Lagrangian': {'x_opt': x_opt_auglag, 'feasible': auglag_feasible, 'time': None, 'iterations': len(traj_auglag), 'final_value': f(x_opt_auglag[0], x_opt_auglag[1])} }\n",
    "        for method, results in constrained_results[point].items():\n",
    "            feasible_str = \"FACTIBLE\" if results['feasible'] else \"NO FACTIBLE\"\n",
    "            print(f\"  {method:20}: {results['iterations']:3d} iter, f(x) = {results['final_value']:.2e} [{feasible_str}]\")\n",
    "    return constrained_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce5b6f8",
   "metadata": {},
   "source": [
    "## 7. Análisis de Rendimiento y Escalabilidad\n",
    "\n",
    "Benchmarking y análisis de escalabilidad para los algoritmos implementados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90cdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_benchmark(f, grad_f, hess_f, algorithms, test_points, n_runs=10):\n",
    "    benchmark_results = {}\n",
    "    for algo_name, algo_func in algorithms.items():\n",
    "        algo_results = {'total_time': 0, 'success_rate': 0, 'avg_iterations': 0, 'avg_precision': 0, 'point_results': {}}\n",
    "        for point in test_points:\n",
    "            point_times = []\n",
    "            point_successes = []\n",
    "            point_iterations = []\n",
    "            point_precisions = []\n",
    "            for run in range(n_runs):\n",
    "                start_time = time.time()\n",
    "                try:\n",
    "                    if algo_name in ['BFGS', 'DFP']:\n",
    "                        x_opt, trajectory = algo_func(f, grad_f, point)\n",
    "                    elif algo_name == 'Trust Region':\n",
    "                        x_opt, trajectory = algo_func(f, grad_f, hess_f, point)\n",
    "                    else:\n",
    "                        x_opt, trajectory = algo_func(point)\n",
    "                    run_time = time.time() - start_time\n",
    "                    final_value = f(x_opt[0], x_opt[1])\n",
    "                    success = final_value < 1e-6\n",
    "                    point_times.append(run_time)\n",
    "                    point_successes.append(success)\n",
    "                    point_iterations.append(len(trajectory))\n",
    "                    point_precisions.append(final_value)\n",
    "                except Exception as e:\n",
    "                    point_times.append(float('inf'))\n",
    "                    point_successes.append(False)\n",
    "                    point_iterations.append(float('inf'))\n",
    "                    point_precisions.append(float('inf'))\n",
    "            algo_results['point_results'][point] = {'avg_time': np.mean(point_times), 'std_time': np.std(point_times), 'success_rate': np.mean(point_successes), 'avg_iterations': np.mean(point_iterations), 'avg_precision': np.mean(point_precisions)}\n",
    "            algo_results['total_time'] += np.sum(point_times)\n",
    "            algo_results['success_rate'] += np.sum(point_successes)\n",
    "            algo_results['avg_iterations'] += np.sum(point_iterations)\n",
    "            algo_results['avg_precision'] += np.sum(point_precisions)\n",
    "        total_points = len(test_points) * n_runs\n",
    "        algo_results['success_rate'] /= total_points\n",
    "        algo_results['avg_iterations'] /= total_points\n",
    "        algo_results['avg_precision'] /= total_points\n",
    "        benchmark_results[algo_name] = algo_results\n",
    "    return benchmark_results\n",
    "\n",
    "def scalability_analysis(f, grad_f, hess_f, base_points, scale_factors):\n",
    "    scalability_results = {}\n",
    "    for scale in scale_factors:\n",
    "        scaled_points = [(scale * x, scale * y) for x, y in base_points]\n",
    "        scale_results = {}\n",
    "        algorithms = {'BFGS': lambda p: bfgs_method(f, grad_f, p), 'DFP': lambda p: dfp_method(f, grad_f, p), 'Trust Region': lambda p: trust_region_dogleg(f, grad_f, hess_f, p)}\n",
    "        for algo_name, algo_func in algorithms.items():\n",
    "            success_count = 0\n",
    "            total_time = 0\n",
    "            total_iterations = 0\n",
    "            for point in scaled_points:\n",
    "                try:\n",
    "                    start_time = time.time()\n",
    "                    x_opt, trajectory = algo_func(point)\n",
    "                    end_time = time.time()\n",
    "                    if f(x_opt[0], x_opt[1]) < 1e-6:\n",
    "                        success_count += 1\n",
    "                    total_time += end_time - start_time\n",
    "                    total_iterations += len(trajectory)\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "            scale_results[algo_name] = {'success_rate': success_count / len(scaled_points), 'avg_time': total_time / len(scaled_points), 'avg_iterations': total_iterations / len(scaled_points)}\n",
    "        scalability_results[scale] = scale_results\n",
    "    return scalability_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e8b535",
   "metadata": {},
   "source": [
    "## 8. Funciones de Resumen y Reportes\n",
    "\n",
    "Funciones para generar reportes y guardar/cargar resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e2e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comprehensive_report(benchmark_results, scalability_results, constrained_results, convergence_data):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"REPORTE COMPREHENSIVO - ANÁLISIS DE OPTIMIZACIÓN\")\n",
    "    print(\"=\" * 80)\n",
    "    # Resumen de benchmark\n",
    "    benchmark_df = []\n",
    "    for algo_name, results in benchmark_results.items():\n",
    "        benchmark_df.append({'Algoritmo': algo_name, 'Tasa Éxito': f\"{results['success_rate']:.2%}\", 'Tiempo Promedio': f\"{results['total_time'] / (len(results['point_results']) * 10):.4f}s\", 'Iteraciones Promedio': f\"{results['avg_iterations']:.1f}\", 'Precisión Promedio': f\"{results['avg_precision']:.2e}\"})\n",
    "    for row in benchmark_df:\n",
    "        print(f\"  {row['Algoritmo']:15} | {row['Tasa Éxito']:8} | {row['Tiempo Promedio']:12} | {row['Iteraciones Promedio']:8} | {row['Precisión Promedio']:12}\")\n",
    "    # Recomendaciones (resumen simplificado)\n",
    "    best_success = max(benchmark_results.items(), key=lambda x: x[1]['success_rate'])\n",
    "    best_time = min(benchmark_results.items(), key=lambda x: x[1]['total_time'])\n",
    "    best_precision = min(benchmark_results.items(), key=lambda x: x[1]['avg_precision'])\n",
    "    print(f\"  Mayor robustez: {best_success[0]} ({best_success[1]['success_rate']:.2%} éxito)\")\n",
    "    print(f\"  Mayor velocidad: {best_time[0]}\")\n",
    "    print(f\"  Mayor precisión: {best_precision[0]}\")\n",
    "\n",
    "def save_results_to_file(results, filename=\"optimization_results_extras.npz\"):\n",
    "    serializable_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            nested_dict = {}\n",
    "            for nested_key, nested_value in value.items():\n",
    "                if hasattr(nested_value, '__iter__') and not isinstance(nested_value, str):\n",
    "                    nested_dict[nested_key] = np.array(nested_value)\n",
    "                else:\n",
    "                    nested_dict[nested_key] = nested_value\n",
    "            serializable_results[key] = nested_dict\n",
    "        elif hasattr(value, '__iter__') and not isinstance(value, str):\n",
    "            serializable_results[key] = np.array(value)\n",
    "        else:\n",
    "            serializable_results[key] = value\n",
    "    np.savez(filename, **serializable_results)\n",
    "    print(f\"Resultados guardados en {filename}\")\n",
    "\n",
    "def load_results_from_file(filename=\"optimization_results_extras.npz\"):\n",
    "    try:\n",
    "        data = np.load(filename, allow_pickle=True)\n",
    "        results = {key: data[key] for key in data.files}\n",
    "        print(f\"Resultados cargados desde {filename}\")\n",
    "        return results\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Archivo {filename} no encontrado\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de864dac",
   "metadata": {},
   "source": [
    "## Ejecución Principal de Experimentos Extendidos\n",
    "\n",
    "Bloque principal para correr experimentos cuando se ejecuta como script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daeba73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso (descomentar y ajustar según la disponibilidad de `f, grad_f, hess_f`)\n",
    "# from optimization_analysis import f, grad_f, hess_f\n",
    "# extended_results = run_extended_experiments(f, grad_f, hess_f)\n",
    "# constrained_results = constrained_optimization_experiments(f, grad_f)\n",
    "# test_points = [(10, 10), (-50, 50), (0.5, 0.5), (-10, -20)]\n",
    "# algorithms = {'BFGS': lambda p: bfgs_method(f, grad_f, p), 'DFP': lambda p: dfp_method(f, grad_f, p), 'Trust Region': lambda p: trust_region_dogleg(f, grad_f, hess_f, p)}\n",
    "# benchmark_results = performance_benchmark(f, grad_f, hess_f, algorithms, test_points, n_runs=5)\n",
    "# base_points = [(1, 1), (2, 2), (5, 5), (10, 10)]\n",
    "# scalability_results = scalability_analysis(f, grad_f, hess_f, base_points, [1,10,100])\n",
    "# convergence_data = {}\n",
    "# generate_comprehensive_report(benchmark_results, scalability_results, constrained_results, convergence_data)\n",
    "# save_results_to_file({'extended_results': extended_results, 'constrained_results': constrained_results})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
